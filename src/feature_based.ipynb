{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install vector database\n",
    "# ! pip install vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from PIL import Image, ImageFont, ImageDraw \n",
    "import torch\n",
    "import clip\n",
    "import json as js\n",
    "from docarray import DocList, BaseDoc\n",
    "from docarray.typing import NdArray\n",
    "import numpy as np\n",
    "from vectordb import InMemoryExactNNVectorDB, HNSWVectorDB\n",
    "from IPython.display import clear_output\n",
    "from natsort import natsorted\n",
    "import pandas as pd\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"ViT-B/32\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "\n",
    "METADATA_PATH = \"../data/metadata/\"\n",
    "KEYFRAME_PATH = \"../data/keyframes/\"\n",
    "FEATURE_PATH = \"../data/features/\"\n",
    "MAP_KEYFRAMES = \"../data/map-keyframes/\"\n",
    "\n",
    "# Workspace for index database\n",
    "WORKSPACE_PATH = \"./workspace_aic23\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-formating Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get video names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_OF_KEYFRAME_NAME = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_names = [name for name in os.listdir(KEYFRAME_PATH) if name != \".gitkeep\"]\n",
    "print(video_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in video_names:\n",
    "    keyframes = [path for path in os.listdir(os.path.join(KEYFRAME_PATH, name))]\n",
    "    for kf in keyframes:\n",
    "        img_name = kf.split(\".\")[0]\n",
    "        if len(img_name) != LEN_OF_KEYFRAME_NAME:\n",
    "            changed_path = os.path.join(KEYFRAME_PATH, name, img_name.zfill(4) + \".jpg\")\n",
    "            old_path = os.path.join(KEYFRAME_PATH, name, kf)\n",
    "            print(f\"Change {old_path} to {changed_path}\")\n",
    "            os.rename(old_path, changed_path)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding():\n",
    "  def __init__(self):\n",
    "    self.device = DEVICE\n",
    "    self.model, _ = clip.load(MODEL, device=self.device)\n",
    "\n",
    "  def __call__(self, text: str) -> np.ndarray:\n",
    "    text_inputs = clip.tokenize([text]).to(self.device)\n",
    "    with torch.no_grad():\n",
    "        text_feature = self.model.encode_text(text_inputs)[0]\n",
    "    return text_feature.detach().cpu().numpy()\n",
    "  \n",
    "  def __call__(self, texts: list[str]) -> np.ndarray:\n",
    "    text_inputs = clip.tokenize(texts).to(self.device)\n",
    "    with torch.no_grad():\n",
    "        text_feature = self.model.encode_text(text_inputs)[0]\n",
    "    return text_feature.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding = TextEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"A blue sky in the background\"\n",
    "query_feat = text_embedding(query)\n",
    "print(len(query_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "querys = [\"A blue sky in the background\", \"People hangout at the beach\", \"Birds are flying in the sky\"]\n",
    "querys_feat = text_embedding(querys)\n",
    "print(len(querys_feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame Document Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameDoc(BaseDoc):\n",
    "  embedding: NdArray[512]\n",
    "  video_name = \"\"\n",
    "  image_path = \"\"\n",
    "  keyframe_id = 0\n",
    "  actual_idx = 0\n",
    "  metadata = {}\n",
    "  \n",
    "  def __str__(self):\n",
    "    return f\"\"\"\n",
    "          Video name: {self.video_name}\n",
    "          Image path: {self.image_path}\n",
    "          Keyframe Id: {self.keyframe_id}\n",
    "          Actual keyframe idx: {self.actual_idx}\n",
    "          Metadata: {self.metadata}\n",
    "          \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Database Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorDB:\n",
    "    text_embedding = TextEmbedding()\n",
    "    backups = []\n",
    "    def __init__(self, workspace, type=\"ANN\"):\n",
    "        #   Approximate Nearest Neighbour based on HNSW algorithm\n",
    "        if type == \"ANN\":\n",
    "            self.DB = HNSWVectorDB[FrameDoc](workspace=workspace)\n",
    "            \n",
    "        # Exhaustive search on the embeddings\n",
    "        else:\n",
    "            self.DB = InMemoryExactNNVectorDB[FrameDoc](workspace=workspace)\n",
    "        \n",
    "        \n",
    "    def index(self, doc_list: List[FrameDoc]):    \n",
    "        # Index database\n",
    "        self.DB.index(inputs=DocList[FrameDoc](doc_list))\n",
    "        \n",
    "    def search(self, query_text: str, topk=100):\n",
    "        query_doc = FrameDoc(embedding=self.text_embedding(query_text))\n",
    "        return self.DB.search(inputs=DocList[FrameDoc]([query_doc]), limit=topk)[0]\n",
    "    \n",
    "    def delete(self, del_doc_list: List[FrameDoc]):\n",
    "        self.DB.delete(docs=DocList[FrameDoc](del_doc_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Needed functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all features files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_feats():\n",
    "    return [os.path.join(FEATURE_PATH, file) for file in os.listdir(FEATURE_PATH) if file.endswith(\".npy\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feat_files = get_all_feats()\n",
    "print(all_feat_files)\n",
    "print(len(all_feat_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create all the Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_docs(npy_files):\n",
    "    doc_list = []\n",
    "    for feat_npy in npy_files:\n",
    "        video_name = feat_npy[feat_npy.find(\"L\"):].split('.')[0]\n",
    "        feats_arr = np.load(os.path.join(feat_npy))\n",
    "        # Load metadata\n",
    "        metadata = {}\n",
    "        with open(os.path.join(METADATA_PATH, video_name + \".json\")) as meta_f:\n",
    "            metadata = js.load(meta_f)\n",
    "            map_kf = pd.read_csv(os.path.join(MAP_KEYFRAMES, video_name + \".csv\"), usecols=[\"frame_idx\"])\n",
    "            \n",
    "            for frame_idx, feat in enumerate(feats_arr):\n",
    "                image_path = os.path.join(KEYFRAME_PATH, video_name, f\"{frame_idx + 1:04d}.jpg\")\n",
    "                doc_list.append(FrameDoc(embedding=feat, video_name=video_name, image_path=image_path, \n",
    "                                        keyframe_id=frame_idx+1, actual_idx=map_kf[\"frame_idx\"][frame_idx], metadata=metadata))\n",
    "                \n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = get_all_docs(all_feat_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(doc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_list[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(result_matches, drawed = None):\n",
    "    images = []\n",
    "    for res in result_matches:\n",
    "        img = Image.open(res.image_path)\n",
    "        if drawed:\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            font = ImageFont.truetype(\"arial.ttf\", 50)\n",
    "            draw.text(xy=(5, 5), text=f\"{res.video_name}, {res.actual_idx}\", align=\"left\", fill=(255,0,0,255), font=font)\n",
    "        images.append(img)\n",
    "        \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(imgs: List[Image.Image]) -> None:\n",
    "    rows = len(imgs) // 2 # see more clearly\n",
    "    if not rows:\n",
    "        rows += 1\n",
    "    cols = len(imgs) // rows\n",
    "    if rows * cols < len(imgs):\n",
    "        rows += 1\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols * w, rows * h))\n",
    "    grid_w, grid_h = grid.size\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
    "\n",
    "    display(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB = VectorDB(\"DB1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi index 1 lan\n",
    "DB.index(doc_list) # ~ 3 mins "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đoạn video về một người phụ nữ mặc áo màu vàng đang bỏ rác vào thùng rác. Thùng rác màu xanh lá đậm và nắp thùng màu đỏ. Rác đang bỏ vào thùng cho biết đó là 1kg baby spinach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = DB.search(\"A woman in yellow shirt is putting trash into the bin\", 5000) # Nên lấy nhiều"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_output()\n",
    "visualize(get_images(results.matches[:50], drawed=True)) # Kha nang la L02_V016,14190 :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB2 = VectorDB(workspace=\"DB2\")\n",
    "DB2.index(results.matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = DB2.search(\"green trash can with red lid\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_output()\n",
    "visualize(get_images(results2.matches[:50], drawed=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB3 = VectorDB(\"DB3\")\n",
    "DB3.index(results2.matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3 = DB3.search(\"A bag is being put in the green trash bin with a red lid by a woman in yellow shirt\", 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(get_images(result_matches=results3.matches[:50], drawed=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB4 = VectorDB()\n",
    "DB4.index(results3.matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results4 = DB4.search(\"A woman in yellow shirt is holding a bag of trash\", 100) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(get_images(result_matches=results4.matches[:50], drawed=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aic23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
